## TPU (v1): Jack of none, Master of one

### The TPU Thesis: Google's Power Play in the AI Arena
In today's world, AI tools and their integration into our daily lives have sparked a surge in computational demands at data centers. One at a time, everything around us will be rewritten by AI. While there's a case to be made for edge AI, most compute today happens in vast data centers.

At the heart of these data centers, [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit) (Graphics Processing Units) reign supreme as the primary computational workhorses powering these AI-driven workloads. The data center GPU market share in 2023 is est to be [$13.1 bil](https://www.gminsights.com/industry-analysis/data-center-gpu-market) and is expected to grow to [$120.5](https://www.gminsights.com/industry-analysis/data-center-gpu-market) bil in 2032. Nvidia currently commands a staggering [92% market share](https://www.benzinga.com/news/24/03/37981249/nvidia-captures-92-of-data-center-gpu-market-underlining-us-leadership-in-generative-ai#:~:text=Nvidia%20capturing%20a%20staggering%2092%25%20market%20share%20in%20data%20center%20GPUs.%C2%A0) in data center GPUs.

Major cloud service providers like AWS, Azure and Google cloud are experiencing increase in ML workloads. But Google, unlike the others, anticipated this as early as 2006. The need to build an Application Specific Integrated Circuit ([ASIC](https://en.wikipedia.org/wiki/Application-specific_integrated_circuit)) to account for the fast growing compute demands of neural networks - you see Google was all in on AI very early on - became urgent in 2013.

Enter the Tensor Processing Unit (TPU). Google just announced its [6th generation](https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus) of TPUs (trillium), but its important for us to understand how it all started, the design philosophy of this chip and why it's integral to Google's edge in AI (pun fully intended) - full vertical integration!

### TPU v1's Dawn
The dire need for a custom chip to handle deep learning workloads, that were already deployed in production in Google products at the time, forced Google to design, verify, build and deploy the chip in its data centers in just 15 months, a process that usually takes several years. Norm Jouppi, one of the principal architects of the MIPS processor, was the tech lead on this project. He [remarked](https://cloud.google.com/blog/products/ai-machine-learning/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu#:~:text=We%20did%20a,it%20was%20hectic.), "we were hiring the team as we were building the chip."

At the time, the TPU delivered 15-30x higher performance and 30-80x higher performance-per-watt than comtemprary CPUs and GPUs.

### The TPU v1's Unconventional Blueprint: Where metal met math
To understand why TPUs were designed the way they were, it's important to have an understand of mathematical operations that undefly neural networks (or ChatGPT for those new to the scene smh).

Each neuron in a neural net (don't worry about understand exactly) does the following calculations:

1. Multiply: input data (x) with weights (w)
2. Add: intermediary results to aggregate the neuron's state
3. Activation function (f): this brings essential non-linearity in an otherwise linear operation of (1.) and (2.)

[Simple Visualization](https://cloud.google.com/blog/products/ai-machine-learning/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu)

![nn viz](https://storage.googleapis.com/gweb-cloudblog-publish/original_images/Calculations_in_Neural_Networkgh5w.GIF)

Neural nets have to operate over millions (today billions) of such values, amounting to massive compute. For their first optimization, rather than operating on bigger 32-bit or 16-bit float (decimal values) operations, the team decided to operate on 8-bit integers instead. Their blog has a great analogy [here](https://cloud.google.com/blog/products/ai-machine-learning/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu#:~:text=If%20it%E2%80%99s%20raining%20outside%2C%20you%20probably%20don%E2%80%99t%20need%20to%20know%20exactly%20how%20many%20droplets%20of%20water%20are%20falling%20per%20second%20%E2%80%94%20you%20just%20wonder%20whether%20it%E2%80%99s%20raining%20lightly%20or%20heavily). This technique is called quanitization, an optimization that uses 8-bit integers to approximate an arbitrary value between a predetermined min and max value.

Quanitization mapping

![quantization viz](https://storage.googleapis.com/gweb-cloudblog-publish/images/tpu-148g2u.max-1500x1500.png)

This integer approximation of float values reduced the hardware footprint and energy consumption. The TPU contained 65,536 8-bit integer multipliers, while the GPUs used widely at the time contained a few thousand 32-bit float multipliers.



### The TPU Legacy: A Glimpse into the Future of AI Acceleration

---

#### PS
I'm only human, a biological language model (maybe little better than that I can give myself more credit), and I may make mistakes in my research. Open to feedback in the issues section of this blog repository.
