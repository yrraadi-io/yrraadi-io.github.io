## TPU (v1): Jack of none, Master of one

### The TPU Thesis: Google's Power Play in the AI Arena
In today's world, AI tools and their integration into our daily lives have sparked a surge in computational demands at data centers. One at a time, everything around us will be rewritten by AI. While there's a case to be made for edge AI, most compute today happens in vast data centers.

At the heart of these data centers, [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit) (Graphics Processing Units) reign supreme as the primary computational workhorses powering these AI-driven workloads. The data center GPU market share in 2023 is est to be [$13.1 bil](https://www.gminsights.com/industry-analysis/data-center-gpu-market) and is expected to grow to [$120.5](https://www.gminsights.com/industry-analysis/data-center-gpu-market) bil in 2032. Nvidia currently commands a staggering [92% market share](https://www.benzinga.com/news/24/03/37981249/nvidia-captures-92-of-data-center-gpu-market-underlining-us-leadership-in-generative-ai#:~:text=Nvidia%20capturing%20a%20staggering%2092%25%20market%20share%20in%20data%20center%20GPUs.%C2%A0) in data center GPUs.

Major cloud service providers like AWS, Azure and Google cloud are experiencing increase in ML workloads. Most cloud providers rely heavily on Nvidia for their supply of GPUs. But Google, unlike the others, anticipated this as early as 2006. The need to build an Application Specific Integrated Circuit ([ASIC](https://en.wikipedia.org/wiki/Application-specific_integrated_circuit)) to account for the fast growing compute demands of neural networks - you see Google was all in on AI very early on - became urgent in 2013.

Enter the Tensor Processing Unit (TPU). Google just announced its [6th generation](https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus) of TPUs (trillium), but its important for us to understand how it all started, the design philosophy of this chip and why it's integral to Google's edge in AI (pun fully intended) - full vertical integration!

### TPU v1's Dawn
The dire need for a custom chip to handle deep learning workloads, that were already deployed in production in Google products at the time, forced Google to design, verify, build and deploy the chip in its data centers in just 15 months, a process that usually takes several years. Norm Jouppi, one of the principal architects of the MIPS processor, was the tech lead on this project. He [remarked](https://cloud.google.com/blog/products/ai-machine-learning/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu#:~:text=We%20did%20a,it%20was%20hectic.), "we were hiring the team as we were building the chip."

At the time, the TPU delivered 15-30x higher performance and 30-80x higher performance-per-watt than comtemprary CPUs and GPUs.

### The TPU v1's Unconventional Blueprint: Where metal met math
To understand why TPUs were designed the way they were, it's important to have an understand of mathematical operations that undefly neural networks (or ChatGPT for those new to the scene smh).

Each neuron in a neural net (don't worry about understanding exactly) does the following calculations:

1. Multiply: input data (*x*) with weights (*w*)
2. Add: intermediary results to aggregate the neuron's state
3. Activation function (*f*): this brings essential non-linearity in an otherwise linear operation of (1.) and (2.)

[*Simple Visualization*](https://cloud.google.com/blog/products/ai-machine-learning/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu)

![nn viz](https://storage.googleapis.com/gweb-cloudblog-publish/original_images/Calculations_in_Neural_Networkgh5w.GIF)

Neural nets have to operate over millions (today billions) of such values, amounting to massive compute. For their first optimization, rather than operating on bigger 32-bit or 16-bit float (decimal values) operations, the team decided to operate on 8-bit integers instead. Their blog has a great analogy [here](https://cloud.google.com/blog/products/ai-machine-learning/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu#:~:text=If%20it%E2%80%99s%20raining%20outside%2C%20you%20probably%20don%E2%80%99t%20need%20to%20know%20exactly%20how%20many%20droplets%20of%20water%20are%20falling%20per%20second%20%E2%80%94%20you%20just%20wonder%20whether%20it%E2%80%99s%20raining%20lightly%20or%20heavily). This technique is called quanitization, an optimization that uses 8-bit integers to approximate an arbitrary value between a predetermined min and max value.

*Quanitization mapping*

![quantization viz](https://storage.googleapis.com/gweb-cloudblog-publish/images/tpu-148g2u.max-1500x1500.png)

This integer approximation of float values reduced the hardware footprint and energy consumption. The TPU contained 65,536 8-bit integer multipliers, while the GPUs used widely at the time contained a few thousand 32-bit float multipliers.

The TPU included the following essential components:

1. Matrix Multiplier Unit (MXU): 65,536 8-bit multiply-add unit for matrix operations.
2. Unified Buffer (UB): 24MB of SRAM working as registers.
3. Activation Unit (AU): Hardwired activation functions.

They chose a [Complex Instruction Set Computer](https://en.wikipedia.org/wiki/Complex_instruction_set_computing) (CISC) style for the TPUs instruction set to run commands.

Google designed the MXU to process hundreds of thousands of operations in a single clock cycle. How? A systolic array.

CPUs were designed to be general purpose chips, which involves storing values in registers and telling the arithmetic-logic units which registers to read values from and operate on. This generality and variety of processing ability comes at a cost in terms of chip area and power consumption.

In an MXU, however, the matrix multiplication operation reuses both inputs and weights many times to produce the output. So each input value can be read once without storing it back in a register. Only spacially adjacent ALUs are connected, making it short and energy efficient. Addition and multiplication is performed in fixed patterns. It's called systolic because the data flows through the chip in waves, like a heart pumping blood.

*Systolic array input vector by weight matrix multiplication*

![systolic array visualization](https://storage.googleapis.com/gweb-cloudblog-publish/original_images/Systolic_Array_for_Neural_Network_1pkw3.GIF)

The TPUs MXU systolic array contains 256x256 = 65,536 ALUs. That's 65,536 8-bit multiply-adds every clock cycle. Since the TPU ran at 700 MHz (700 x 10^6 clock cycles per second) it could run 46 x 10^12 multiple-adds every second or 92 Teraops per second. Mind blowing! In contrast, the CPU could do a few operations per cycle (not second), GPU could do tens of thousands and the TPU hundreds of thousands.

### The TPU Legacy: A Glimpse into the Future of AI Acceleration
Today, Google has announced their 6th generation of trillium TPU v6 chips. This custom silicon, among other things, gives Google an edge above other players in AI. They're clearly not as dependent on Nvidia GPUs as are other companies. Google's latest Gemini models are completely trained on their own TPUs.

In later articles, I'll dive deeper into developments across subsequent version of the TPU leading to the latest. The TPU is Google's edge in AI.

### Sources
[An in-depth look at Googleâ€™s first Tensor Processing Unit (TPU)](https://cloud.google.com/blog/products/ai-machine-learning/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu)

[Quantifying the performance of the TPU, our first machine learning chip](https://cloud.google.com/blog/products/gcp/quantifying-the-performance-of-the-tpu-our-first-machine-learning-chip/)

[In-Datacenter Performance Analysis of a Tensor Processing Unit
TM](https://arxiv.org/pdf/1704.04760)

[Announcing Trillium, the sixth generation of Google Cloud TPU](https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus)

[Gemini](https://gemini.google.com/)

---

#### PS
I'm only human, a biological language model (maybe little better than that I can give myself more credit), and I may make mistakes in my research. Open to feedback in the issues section of this blog repository.
